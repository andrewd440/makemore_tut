{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 words\n",
      "15 max word length\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "print(len(words), 'words')\n",
    "print(max(len(word) for word in words), 'max word length')\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27 characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {w: i+1 for i, w in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size, 'characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "device = 'cuda'\n",
    "\n",
    "def build_dataset(words):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word in words:\n",
    "        ctx = [0] * block_size\n",
    "        for c in word + '.':\n",
    "            X.append(ctx)\n",
    "            Y.append(stoi[c])\n",
    "            #print(''.join(itos[x] for x in ctx), '--->',c)\n",
    "            ctx = ctx[1:] + [stoi[c]]\n",
    "\n",
    "    X = torch.tensor(X, device=device)\n",
    "    Y = torch.tensor(Y, device=device)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "n1 = int(len(words) * .8)\n",
    "n2 = int(len(words) * .9)\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ---> y\n",
      "..y ---> u\n",
      ".yu ---> h\n",
      "yuh ---> e\n",
      "uhe ---> n\n",
      "hen ---> g\n",
      "eng ---> .\n",
      "... ---> d\n",
      "..d ---> i\n",
      ".di ---> o\n",
      "dio ---> n\n",
      "ion ---> d\n",
      "ond ---> r\n",
      "ndr ---> e\n",
      "dre ---> .\n",
      "... ---> x\n",
      "..x ---> a\n",
      ".xa ---> v\n",
      "xav ---> i\n",
      "avi ---> e\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(Xtrain[:20], Ytrain[:20]):\n",
    "    print(''.join(itos[x_.item()] for x_ in x), '--->', itos[y.item()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, device='cpu', bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), device=device) / fan_in**0.5\n",
    "        self.b = torch.zeros(fan_out, device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.b is None else [self.b])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, fan_in, eps=1e-5, momentum=0.1, device='cpu'):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.gamma = torch.ones(fan_in, device=device)\n",
    "        self.beta = torch.zeros(fan_in, device=device)\n",
    "        self.mean_running = torch.zeros(fan_in, device=device)\n",
    "        self.var_running = torch.ones(fan_in, device=device)\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(0, keepdim=True)\n",
    "            var = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            mean = self.mean_running\n",
    "            var = self.var_running\n",
    "\n",
    "        self.out = (x - mean) / (var + self.eps).sqrt()\n",
    "        self.out = self.out * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.mean_running = self.momentum * self.mean_running + (1 - self.momentum) * mean\n",
    "                self.var_running = self.momentum * self.var_running + (1 - self.momentum) * var\n",
    "\n",
    "        return self.out \n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, embedding_size, device='cpu'):\n",
    "        self.weight = torch.randn((vocab_size, embedding_size), device=device)\n",
    "\n",
    "    def __call__(self, ix):\n",
    "        self.out = self.weight[ix]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d470594d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 12097\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embd, device=device),\n",
    "    Flatten(),\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False, device=device), BatchNorm1d(n_hidden, device=device), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, device=device),\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1\n",
    "\n",
    "print(\"Model Params:\", sum(p.nelement() for p in model.parameters()))\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2972\n",
      "  10000/ 200000: 2.0884\n",
      "  20000/ 200000: 2.2309\n",
      "  30000/ 200000: 2.1892\n",
      "  40000/ 200000: 1.8902\n",
      "  50000/ 200000: 2.3622\n",
      "  60000/ 200000: 2.4263\n",
      "  70000/ 200000: 2.5687\n",
      "  80000/ 200000: 1.8453\n",
      "  90000/ 200000: 2.2090\n",
      " 100000/ 200000: 2.3640\n",
      " 110000/ 200000: 2.0855\n",
      " 120000/ 200000: 2.2904\n",
      " 130000/ 200000: 2.1505\n",
      " 140000/ 200000: 2.1970\n",
      " 150000/ 200000: 2.2652\n",
      " 160000/ 200000: 1.8880\n",
      " 170000/ 200000: 2.3349\n",
      " 180000/ 200000: 2.1135\n",
      " 190000/ 200000: 2.2555\n",
      "tensor(1.9021, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 200000\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtrain.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtrain[ix], Ytrain[ix]\n",
    "\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    #-- backprop\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #learning_rate = lrs[i]\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    lossi.append(loss.log10().item())\n",
    "    # if i >= 1:\n",
    "    #     break\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mplot(torch\u001b[39m.\u001b[39mtensor(lossi)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor(3.2701, device='mps:0')\n",
      "dev tensor(3.2695, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtrain, Ytrain),\n",
    "        'dev': (Xdev, Ydev),\n",
    "        'test': (Xtest, Ytest),\n",
    "    }[split]\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss\n",
    "\n",
    "loss_train = split_loss('train')\n",
    "print('train', loss_train)\n",
    "\n",
    "loss_dev = split_loss('dev')\n",
    "print('dev', loss_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spuvsxxlzzxgrrgedumn.\n",
      "vyaez.\n",
      "fasadwfha.\n",
      "qclnylteiyqevhycucxuxayxigsifpeqyqxczebmzpecjmbdigjfnrkvugnivdcwkeanjrodzswadpbnnmjeijdcgigduozyylouzxcoqznkomageatgrqqznryogbrmbnwvqblemmhektgjqtz.\n",
      "ahxh.\n",
      "ryhvwvsbhmgovboebsmfmjcvlelkrhkqcubjbdtabo.\n",
      ".\n",
      "lvelglfszvolddoavqrquzihwhmytrryimazkdgvyfbfulvgvtvfiygfsextmzxonlcyakjzeqfzskmnqgpvqvqoxskjpaqhufpfqorboqck.\n",
      "dxugttkpvmvwwavggnaxcbscjjbnblhvlsguzmgycvgausvishjwx.\n",
      "rnqhecded.\n",
      "kumaulbfcl.\n",
      "thqblru.\n",
      "ackxwvnlnzowhrerbnisawshigookqoynzqoruoywppcvizdeqlniler.\n",
      "dhjikxrumskk.\n",
      "htpgcarhmjxezbagpgvuwwcejreazxlfpoylhpkkbjvwhchwigrkbuozkjwuffuttwnxaewtvppwyjnscnfyot.\n",
      "rkexjkijawurajajkoqsyaxqnrqhuhxfdb.\n",
      "dtkyebvjgocdcxbosvxwbssvjygq.\n",
      "qblxzsiyrhnfypahohjgabjjjaufluehmgnmrx.\n",
      "rxjjalaznlzmzgxkwmlcpbxlsuadjebvoddpitukhqxqdogukzxivrwagaaqnhuemhlvsshpol.\n",
      "milmbikiefdlfykjmmvahjluqaictkf.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "\n",
    "      logits = model(torch.tensor([context]))\n",
    "      # prediction\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\" == \"79f7de7a1a48f82b08b2d7f16503daf9de7de7423f4e70372a30919b9cbe35b6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5a1ac80b7935826bd9522a6752671698b12798f24a5ff02276dcd363303f4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
